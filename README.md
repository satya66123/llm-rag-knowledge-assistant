# llm-rag-knowledge-assistant

## ğŸ¤– LLM RAG Knowledge Assistant (End-to-End Project)

This repository contains a step-by-step implementation of a **Retrieval-Augmented Generation (RAG)** system built from scratch using Python.

âœ… Built incrementally like real AI engineers:
- first build retrieval foundation (embeddings + vector search)
- then build chunk-based retrieval
- then build context + prompt packaging
- then integrate OpenAI
- then expose the system via FastAPI + Streamlit Chat UI

---

# âœ… Progress Timeline (Day-wise)

## ğŸ“… Day 1 â€” FAISS Embeddings Pipeline âœ…

### ğŸ¯ Goal
Build the **core semantic memory layer**, which is the foundation of every RAG system.

### âœ… Implemented
- âœ… Python project setup (clean structure)
- âœ… SentenceTransformer model download + load
- âœ… Text embedding generation
- âœ… FAISS index creation (vector database behavior)
- âœ… Semantic search working end-to-end

### âœ… Output Verified
- Query embeddings generated
- Top relevant sentence retrieved
- No hidden errors (exit code 0)

### ğŸ“„ Script
Run:
```bash
python text_embeddings.py
ğŸ“… Day 2 â€” Document Loading + Chunking + Retrieval âœ…
ğŸ¯ Goal
Move from toy examples to a real RAG retriever, by embedding & searching chunks (not full documents).

âœ… Implemented
âœ… Load .txt documents from data/sample_docs/

âœ… Document loader (app/loader.py)

âœ… Chunking with overlap (app/chunking.py)

âœ… Embed each chunk

âœ… Store chunk metadata:

chunk_id

doc_id

source

text

âœ… Retrieve Top-K relevant chunks

âœ… Output Verified
Loaded 3 docs

Created multiple chunks

Query like "What is RAG?" returns correct chunk from RAG document

ğŸ“„ Script
Run:
python rag_retrieval_day2.py

ğŸ“… Day 3 â€” FULL SYSTEM COMPLETION (4 Parts) âœ…âœ…âœ…âœ…
Day 3 is the final sprint day where the project was completed to 100%.

âœ… Day 3 â€” Part 1: Context Builder + Prompt Builder âœ…
ğŸ¯ Goal
Convert raw retrieved chunks into an LLM-ready context pack, and build a clean instruction prompt.

âœ… Implemented
ğŸ“„ app/prompts.py

Includes:

âœ… build_context(retrieved_chunks)

formats chunks into structured context

includes source/doc/chunk ids for traceability

âœ… build_prompt(question, context)

prompt template that forces grounded answer:

â€œAnswer using ONLY the provided contextâ€

If not in context â†’ â€œI donâ€™t knowâ€

âœ… Why This Is Important
This is what separates real RAG from just semantic search.
It ensures:
âœ… grounded answers
âœ… reduced hallucinations
âœ… source traceability

âœ… Day 3 â€” Part 2: Persistence (Save/Load FAISS Index) âœ…
ğŸ¯ Goal
Make the project production-ready by avoiding re-embedding every run.

âœ… Implemented
ğŸ“„ app/persistence.py

âœ… Save FAISS index to disk (faiss.index)

âœ… Save metadata as JSON (metadata.json)

âœ… Load index + metadata on startup

âœ… Output Verified
First run builds index and saves

Future runs load instantly from disk

âœ… Day 3 â€” Part 3: Offline RAG Ask CLI Pipeline âœ…
ğŸ¯ Goal
Create a full offline RAG pipeline:

Question â†’ Retrieval â†’ Context â†’ Prompt

âœ… Implemented
ğŸ“„ rag_ask_cli.py

Features:

âœ… Asks question in terminal

âœ… Retrieves top-k chunks

âœ… Builds context pack

âœ… Prints final prompt (LLM-ready)

âœ… Shows sources (doc/chunk ids)

âœ… Why This Matters
This proves the pipeline is correct even before OpenAI integration.

âœ… Day 3 â€” Part 4: OpenAI + FastAPI + Streamlit UI (100% Completion) âœ…
ğŸ¯ Goal
Complete a real usable RAG product:

Question â†’ RAG retrieval â†’ OpenAI answer â†’ API response â†’ Chat UI

âœ… Implemented
âœ… OpenAI LLM Answering
ğŸ“„ app/llm.py

calls OpenAI API

returns grounded answer

ğŸ“„ app/rag.py

full RAG function:

retrieve chunks

build context

build prompt

call OpenAI

return answer + sources

âœ… FastAPI Backend
ğŸ“„ app/main.py

Endpoints:

âœ… GET /health

âœ… POST /ask

Response includes:

answer

sources used (doc/chunk ids + text)

Swagger docs:

âœ… /docs

âœ… Streamlit Chat UI
ğŸ“„ ui_streamlit.py

UI Features:

âœ… Chat-style interface

âœ… Sidebar controls:

top_k

show sources toggle

show full source text toggle

enable chat memory toggle

âœ… Clear chat button

âœ… Displays answer + expandable sources

ğŸ“‚ Final Project Structure
text
Copy code
llm-rag-knowledge-assistant/
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ embeddings.py
â”‚   â”œâ”€â”€ loader.py
â”‚   â”œâ”€â”€ chunking.py
â”‚   â”œâ”€â”€ prompts.py
â”‚   â”œâ”€â”€ persistence.py
â”‚   â”œâ”€â”€ llm.py
â”‚   â”œâ”€â”€ rag.py
â”‚   â””â”€â”€ main.py
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ sample_docs/
â”‚       â”œâ”€â”€ doc1.txt
â”‚       â”œâ”€â”€ doc2.txt
â”‚       â””â”€â”€ doc3.txt
â”‚
â”œâ”€â”€ storage/
â”‚   â”œâ”€â”€ faiss.index
â”‚   â””â”€â”€ metadata.json
â”‚
â”œâ”€â”€ text_embeddings.py
â”œâ”€â”€ rag_retrieval_day2.py
â”œâ”€â”€ rag_ask_cli.py
â”œâ”€â”€ rag_ask_openai.py
â”œâ”€â”€ ui_streamlit.py
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore
âš™ï¸ Setup & Installation
1) Install dependencies

pip install -r requirements.txt
ğŸ” Environment Variable
Set your OpenAI key:

âœ… PowerShell (temporary for session)
powershell
Copy code
$env:OPENAI_API_KEY="your_api_key_here"
â–¶ï¸ Run Instructions
âœ… Run FastAPI Server

uvicorn app.main:app --reload
Swagger docs: http://127.0.0.1:8000/docs

Health check: http://127.0.0.1:8000/health

âœ… Run Streamlit UI
(keep FastAPI running in another terminal)


streamlit run ui_streamlit.py
UI will open at:

http://127.0.0.1:8501

ğŸ† Final Outcome
âœ… A complete end-to-end RAG Chat Assistant system:

chunk-based retrieval with FAISS

grounded OpenAI answering

FastAPI backend

Streamlit chat interface

sources shown for transparency

ğŸ§‘â€ğŸ’» Author: satya66123 -satyasrinath653512@gmail.com
Built using a disciplined, real engineering approach (Day-wise incremental development).

---

If you want, I can also generate:
âœ… **README with screenshots section**  
âœ… A polished **â€œProject Demoâ€ section**  
âœ… A final **LinkedIn post** with Day 1/2/3 breakdown + GitHub link